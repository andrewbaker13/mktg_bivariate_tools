0. High-Level Overview

You are building a web app for teaching choice-based conjoint (CBC).

Front end: HTML + JavaScript only (no frameworks required, allowed if helpful).

Back end: Python is allowed and is preferred for estimation if needed.

Core capabilities:

Upload CBC data (long format).

Estimate individual-level utilities (part-worths) from choice data.

Handle:

Categorical attributes

Numeric attributes (as categorical by default; optional linear/quadratic)

A special PRICE attribute

A None alternative

Competitor alternatives modeled as whole alternatives with their own constants.

Run simulations (shares, profit) using these utilities.

Segment respondents using k-means on normalized utilities.

Visualize results in the browser.

The app is for students; clarity and transparency are more important than extreme performance or sophistication.

1. Data Model and Input Specification
1.1. Conjoint Data Format (CSV)

The main input is a long-format CSV where each row corresponds to one alternative in one choice task for one respondent.

Required columns:

respondent_id

String or integer.

Identifies the respondent.

task_id

String or integer.

Identifies the choice task (within respondent).

alternative_id

String or integer.

Identifies the alternative within a task.

May include:

Systematic experimental alternatives (e.g., A, B, C)

Named competitor alternatives (e.g., "iPhone", "SamsungS20")

A special alternative representing â€œNoneâ€ (user selects label in UI).

chosen

Binary: 0/1.

1 for the alternative chosen by the respondent in that task; 0 otherwise.

Attribute Columns (one column per attribute, NOT dummy coded)

Examples:

brand (e.g., "BrandX", "BrandY", "BrandZ", "CompetitorApple")

color (e.g., "Black", "White", "Gold")

storage_gb (numeric)

price (numeric)

No pre-generated dummies are assumed. The app handles coding.

Missing or unused attributes for some alternatives (e.g., competitor or None) can be empty/NA; app must handle that gracefully.

1.2. Attribute Metadata (Defined in UI)

After upload, the user configures a metadata panel specifying:

For each attribute column:

Attribute name (read from header, editable in UI).

Attribute type:

categorical

numeric_linear

numeric_quadratic

price (special case, numeric)

Optional:

Order of levels (for categorical)

Whether to include attribute in simulation (can be toggled off if irrelevant)

Defaults:

All attributes are initially treated as categorical.

The user must explicitly mark numeric variables as:

numeric_linear or

numeric_quadratic or

price.

1.3. Special Fields: PRICE

There may be one attribute explicitly marked as price.

If present in the data:

It is modeled as a numeric variable.

Included in utilities and simulations.

If NOT present in the data:

The simulation UI must allow the user to enter price values for each product in a scenario.

The model will still have a single price coefficient per respondent estimated from tasks where price varied (if price existed) or may be absent (in which case price-based simulation is limited).

1.4. Special Alternatives
1.4.1. None Alternative

The user will select which alternative_id corresponds to â€œNone/No choiceâ€.

For each respondent, the model estimates a unique utility parameter for â€œNoneâ€:

ğ›½
none
,
ğ‘–
Î²
none,i
	â€‹


This parameter is included in simulations.

1.4.2. Competitor Alternatives (Approach B)

Competitor alternatives are whole alternatives, not decomposed into attribute-level utilities beyond what is in the data.

Each distinct competitor alternative (e.g., "iPhone", "SamsungS20") is treated as having its own alternative-specific constant (ASC) per respondent:

ğ›½
comp
ğ‘˜
,
ğ‘–
Î²
comp
k
	â€‹

,i
	â€‹

 for competitor k and respondent i.

Conceptually:

Experimental alternatives are explained by attribute-level utilities.

Each competitor gets an extra â€œbrand advantage/disadvantageâ€ via its ASC.

Competitor alternatives may or may not have attribute values (e.g., price) in the data.

If they do have price in the tasks, price contributes to their utility via the estimated price coefficient.

If not, their utility is driven solely by the competitor-specific constant.

Key limitation:
Competitorsâ€™ attribute structure is not fully modeled (no full decomposition across attributes). Their main role is to provide a realistic constant benchmark in simulations.

2. Estimation Model
2.1. Core Approach

Back end: Python service.

For each respondent 
ğ‘–
i, estimate a multinomial logit (MNL) / conditional logit model with:

Alternative-specific constants for:

Competitor alternatives

None alternative

Optionally, a base product alternative (if desired)

Attribute coefficients based on attribute type and coding.

Optional L2 (ridge) regularization to stabilize estimates, especially when tasks per respondent ~8â€“10.

The goal is to get individual-level utilities suitable for teaching & simulation.

2.2. Coding Rules

For each respondent:

Categorical attributes

Automatically coded into dummy or effects codes.

For simplicity and transparency, choose dummy coding by default:

One level is baseline.

Each non-baseline level gets its own coefficient.

(Optionally: include a flag in code to switch to effects coding; not essential in v1.)

Numeric_linear

A single slope coefficient per attribute:

ğ‘ˆ
ğ‘
ğ‘¡
ğ‘¡
ğ‘Ÿ
(
ğ‘¥
)
=
ğ›½
ğ‘
ğ‘¡
ğ‘¡
ğ‘Ÿ
,
ğ‘–
â‹…
ğ‘¥
U
attr
	â€‹

(x)=Î²
attr,i
	â€‹

â‹…x

Numeric_quadratic

Two coefficients per attribute:

ğ‘ˆ
ğ‘
ğ‘¡
ğ‘¡
ğ‘Ÿ
(
ğ‘¥
)
=
ğ›½
1
,
ğ‘
ğ‘¡
ğ‘¡
ğ‘Ÿ
,
ğ‘–
â‹…
ğ‘¥
+
ğ›½
2
,
ğ‘
ğ‘¡
ğ‘¡
ğ‘Ÿ
,
ğ‘–
â‹…
ğ‘¥
2
U
attr
	â€‹

(x)=Î²
1,attr,i
	â€‹

â‹…x+Î²
2,attr,i
	â€‹

â‹…x
2

Price

Treated as numeric_linear for estimation:

ğ‘ˆ
ğ‘
ğ‘Ÿ
ğ‘–
ğ‘
ğ‘’
(
ğ‘
)
=
ğ›½
price
,
ğ‘–
â‹…
ğ‘
U
price
	â€‹

(p)=Î²
price,i
	â€‹

â‹…p

Sign convention: expect negative.

This coefficient is crucial for:

Simulations

Profit modeling

Willingness-to-pay explanations (later).

Alternative-specific constants (ASCs)

Each competitor alternative 
ğ‘˜
k gets:

ğ›½
comp
ğ‘˜
,
ğ‘–
Î²
comp
k
	â€‹

,i
	â€‹


â€œNoneâ€ alternative gets:

ğ›½
none
,
ğ‘–
Î²
none,i
	â€‹


One alternative (e.g., the first experimental alternative) is chosen as baseline and gets ASC = 0.

2.3. Utility Function

For respondent 
ğ‘–
i, alternative 
ğ‘—
j, and task 
ğ‘¡
t:

ğ‘ˆ
ğ‘–
ğ‘—
ğ‘¡
=
ğ›¼
ğ‘—
,
ğ‘–
+
âˆ‘
ğ‘
âˆˆ
cat
âˆ‘
ğ‘™
ğ›½
ğ‘
,
ğ‘™
,
ğ‘–
â‹…
ğ¼
(
level
(
ğ‘
)
=
ğ‘™
)
+
âˆ‘
ğ‘
âˆˆ
num-linear
ğ›½
ğ‘
,
ğ‘–
â‹…
ğ‘¥
ğ‘
+
âˆ‘
ğ‘
âˆˆ
num-quad
(
ğ›½
ğ‘
1
,
ğ‘–
â‹…
ğ‘¥
ğ‘
+
ğ›½
ğ‘
2
,
ğ‘–
â‹…
ğ‘¥
ğ‘
2
)
+
ğ›½
price
,
ğ‘–
â‹…
ğ‘
ğ‘—
U
ijt
	â€‹

=Î±
j,i
	â€‹

+
aâˆˆcat
âˆ‘
	â€‹

l
âˆ‘
	â€‹

Î²
a,l,i
	â€‹

â‹…I(level(a)=l)+
bâˆˆnum-linear
âˆ‘
	â€‹

Î²
b,i
	â€‹

â‹…x
b
	â€‹

+
câˆˆnum-quad
âˆ‘
	â€‹

(Î²
c1,i
	â€‹

â‹…x
c
	â€‹

+Î²
c2,i
	â€‹

â‹…x
c
2
	â€‹

)+Î²
price,i
	â€‹

â‹…p
j
	â€‹


ğ›¼
ğ‘—
,
ğ‘–
Î±
j,i
	â€‹

 includes:

0 for baseline product

ASCs for competitors and None

2.4. Choice Probabilities

Conditional logit:

ğ‘ƒ
(
choose 
ğ‘—
âˆ£
task 
ğ‘¡
,
ğ‘–
)
=
exp
â¡
(
ğ‘ˆ
ğ‘–
ğ‘—
ğ‘¡
)
âˆ‘
ğ‘˜
âˆˆ
ğ¶
ğ‘–
ğ‘¡
exp
â¡
(
ğ‘ˆ
ğ‘–
ğ‘˜
ğ‘¡
)
P(choose jâˆ£task t,i)=
âˆ‘
kâˆˆC
it
	â€‹

	â€‹

exp(U
ikt
	â€‹

)
exp(U
ijt
	â€‹

)
	â€‹


Where 
ğ¶
ğ‘–
ğ‘¡
C
it
	â€‹

 is the choice set for respondent 
ğ‘–
i in task 
ğ‘¡
t.

2.5. Estimation Details (for the dev LLM)

Backend stack suggestion:

Python 3.x

pandas for data handling

numpy for numerical ops

scikit-learn or statsmodels for logistic regression OR custom MNL optimizer.

Estimation strategy (per respondent):

Filter the long data to single respondent.

Build the design matrix 
ğ‘‹
ğ‘–
X
i
	â€‹

 with:

Columns for all attribute codes and price.

Columns for ASCs (competitors + none).

Encode outcome as chosen alternative per task; use one-row-per-alternative logit formulation.

Fit MNL/conditional logit using maximum likelihood.

Apply ridge regularization (e.g., L2 penalty) to reduce overfitting/instability.

Return:

Coefficient vector for respondent i

Log-likelihood, pseudo-RÂ², etc.

Parallelization:

Respondent-level models can be run in parallel if performance needed.

2.6. Outputs from Estimation

Backend returns:

Respondent-level utilities

For each respondent i:

Coefficients for:

Each attribute level (or numeric term)

Price coefficient

None ASC

Each competitor ASC

Attribute Importance (per respondent)

For each attribute:

Range of total utility across its levels or across typical values.

Importance = (range for attribute) / (sum of ranges for all attributes).

Fit statistics (optional but ideal):

Log-likelihood at convergence

Null log-likelihood

McFadden pseudo-RÂ²

Number of tasks/alternatives used

Aggregated summaries:

Mean utilities across respondents

Mean attribute importances

Distribution summaries (mean, median, SD)

These outputs are passed back to the front end as JSON for visualization and simulation.

3. Simulation Engine (Front-End)
3.1. Scenario Definition UI

Students create simulation scenarios in the browser.

Each scenario consists of:

A list of alternatives (products) in the simulated market.

For each alternative:

name (e.g., â€œOur Product Aâ€)

Attribute values (for categorical and numeric attributes)

price value (if applicable)

cost value (for profit calculation)

type:

"our_product"

"competitor"

"none" (no attributes, no price)

Important:

Competitor alternatives used in simulation should map to the ASCs from the estimation:

E.g. a competitor named "iPhone" uses the coefficient 
ğ›½
comp
iPhone
,
ğ‘–
Î²
comp
iPhone
	â€‹

,i
	â€‹

 for each respondent.

For â€œNoneâ€, use its ASC 
ğ›½
none
,
ğ‘–
Î²
none,i
	â€‹

.

3.2. Utility and Choice Probability in Simulation

Given respondent-level coefficients and a scenario:

For each respondent 
ğ‘–
i and alternative 
ğ‘—
j in scenario:

Compute 
ğ‘ˆ
ğ‘–
ğ‘—
U
ij
	â€‹

 using:

attribute utilities (for our products)

competitor ASC (if competitor)

None ASC (if none)

price effect (if price exists).

Compute choice probabilities:

ğ‘ƒ
ğ‘–
ğ‘—
=
exp
â¡
(
ğ‘ˆ
ğ‘–
ğ‘—
)
âˆ‘
ğ‘˜
exp
â¡
(
ğ‘ˆ
ğ‘–
ğ‘˜
)
P
ij
	â€‹

=
âˆ‘
k
	â€‹

exp(U
ik
	â€‹

)
exp(U
ij
	â€‹

)
	â€‹


Market share (preference share) for each alternative j:

Average across respondents:

ğ‘†
â„
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘—
=
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
ğ‘ƒ
ğ‘–
ğ‘—
Share
j
	â€‹

=
N
1
	â€‹

i=1
âˆ‘
N
	â€‹

P
ij
	â€‹


If segments exist (see section 4), compute segment-level shares analogously.

All of this runs in the front-end using the JSON utilities from the backend.

3.3. Profit Calculation

For each alternative j:

The user provides:

price_j

cost_j

Per-unit profit: 
ğ‘š
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘–
ğ‘›
ğ‘—
=
ğ‘
ğ‘Ÿ
ğ‘–
ğ‘
ğ‘’
ğ‘—
âˆ’
ğ‘
ğ‘œ
ğ‘ 
ğ‘¡
ğ‘—
margin
j
	â€‹

=price
j
	â€‹

âˆ’cost
j
	â€‹


Expected profit (up to scale) per respondent group:

ğ‘ƒ
ğ‘Ÿ
ğ‘œ
ğ‘“
ğ‘–
ğ‘¡
ğ‘—
=
ğ‘†
â„
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘—
â‹…
ğ‘€
ğ‘
ğ‘Ÿ
ğ‘˜
ğ‘’
ğ‘¡
ğ‘†
ğ‘–
ğ‘§
ğ‘’
â‹…
ğ‘š
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘–
ğ‘›
ğ‘—
Profit
j
	â€‹

=Share
j
	â€‹

â‹…MarketSizeâ‹…margin
j
	â€‹


MarketSize can be a user-entered number (e.g., 10,000 customers).

Display:

Profit by product

Total profit

Profit vs. price plots (if user sweeps price).

3.4. Brute-Force Optimization (First Version)

For our products only:

User chooses:

Which attributes to vary.

Allowed levels/values for each.

Range of prices (if price is included).

App generates all candidate configurations (Cartesian product).

For each configuration (or set of configurations if multiple products):

Compute shares & profit as above.

Record results.

Display:

Top K configurations by:

Market share

Profit

Plots of share/profit vs attribute levels/price.

No sophisticated search needed initially.

4. Segmentation (Front-End)

Once respondent-level utilities are estimated:

Build a feature vector per respondent:

All standardized part-worths (including price coefficient, optionally excluding ASCs).

Normalize or standardize each coefficient (z-score) across respondents.

Run k-means clustering (front-end JS implementation) on these feature vectors.

User selects number of clusters (e.g., 2â€“6).

Output:

Segment membership per respondent.

Segment-level average part-worths.

Segment-level attribute importances.

Segment-level simulation results (shares, profit when scenario is run per segment).

Visualize:

Radar charts or grouped bar charts of utilities by segment.

Segment size pie chart.

5. Front-End UI Flow
5.1. Step 0 â€” Landing Page

Short explanation of:

What CBC is

What the tool does

Buttons:

â€œUpload Data & Estimate Modelâ€

â€œLoad Demo Datasetâ€

5.2. Step 1 â€” Upload Data

File upload control (CSV).

Preview table (first 20 rows).

Mapping step:

Confirm respondent_id, task_id, alternative_id, chosen.

Auto-detect attribute columns (all other columns).

Validation:

Check at least one choice per task.

Check each task has â‰¥2 alternatives.

Warn if respondents have fewer than ~8 tasks.

5.3. Step 2 â€” Attribute Setup

For each attribute column:

Dropdown â€œAttribute Typeâ€:

Categorical

Numeric (linear)

Numeric (quadratic)

Price (special)

For categorical:

Show list of levels and allow reordering.

Pick:

â€œWhich alternative_id is the None option?â€

â€œWhich alternative_ids are competitors (if any)?â€

Store this metadata as a JSON object and send with data to the backend.

5.4. Step 3 â€” Estimation

Button: â€œEstimate Utilitiesâ€

Sends:

Filtered CSV (or JSON representation)

Attribute metadata

Model options (regularization strength, etc.)

To backend /estimate endpoint.

Show progress indicator.

When done:

Display success + summary:

respondents estimated

Average pseudo-RÂ²

Time taken

5.5. Step 4 â€” Results & Visualization

Tabs:

Individual-Level Utilities

Table: respondent Ã— attribute-level utilities.

Download CSV.

Aggregated Attribute Importance

Bar chart of mean importance per attribute.

None & Competitor Effects

Distribution of Î²_none and competitor ASCs.

Histograms or boxplots.

5.6. Step 5 â€” Segmentation

Control for k (number of clusters).

Run k-means.

Show:

Segment sizes.

Segment-level attribute importances.

Segment-level price sensitivity.

5.7. Step 6 â€” Simulation

Scenario builder:

Define alternatives with attributes and prices/costs.

Flag alternatives as:

Our product

Competitor (mapped by name to competitor ASC)

None (automatic mapping)

Buttons:

â€œRun Simulationâ€

â€œRun Optimizationâ€ (brute force search across allowed combos).

Show outputs:

Share by alternative (overall and by segment).

Profit by alternative and total.

Downloadable CSV of simulation results.

Allow saving multiple scenarios for side-by-side comparison.

6. Backend API Contract (Python Service)

At minimum, define:

6.1. /estimate (POST)

Request JSON:

{
  "data": [
    {
      "respondent_id": "R1",
      "task_id": "1",
      "alternative_id": "A",
      "chosen": 1,
      "brand": "BrandX",
      "color": "Black",
      "storage_gb": 64,
      "price": 699
    }
    // ... more rows
  ],
  "attribute_metadata": {
    "brand": {"type": "categorical"},
    "color": {"type": "categorical"},
    "storage_gb": {"type": "numeric_linear"},
    "price": {"type": "price"}
  },
  "none_alternative_id": "None",
  "competitor_alternative_ids": ["iPhone", "SamsungS20"],
  "model_options": {
    "regularization": "L2",
    "reg_strength": 1.0
  }
}


Response JSON (simplified structure):

{
  "respondents": [
    {
      "respondent_id": "R1",
      "coefficients": {
        "brand_BrandY": 0.3,
        "brand_BrandZ": -0.1,
        "color_White": 0.2,
        "storage_gb": 0.01,
        "price": -0.005,
        "ASC_None": -1.2,
        "ASC_Competitor_iPhone": 0.8,
        "ASC_Competitor_SamsungS20": 0.5
      },
      "attribute_importance": {
        "brand": 0.4,
        "color": 0.1,
        "storage_gb": 0.2,
        "price": 0.3
      },
      "fit": {
        "log_likelihood": -120.0,
        "null_log_likelihood": -150.0,
        "pseudo_r2": 0.2
      }
    }
    // more respondents
  ],
  "aggregate_summaries": {
    "mean_attribute_importance": {
      "brand": 0.35,
      "color": 0.15,
      "storage_gb": 0.2,
      "price": 0.3
    }
  }
}


No other endpoints are strictly required for v1; simulation and segmentation can be entirely front-end using this output.

7. Performance & Limits

Target data scale:

Respondents: ~200â€“500

Tasks per respondent: 8â€“30

Alternatives per task: 2â€“5

Estimation will run on the backend; front end just handles visuals and simulations.

Brute-force optimization may need guardrails:

Warn if total configurations exceed some threshold (e.g., 5,000â€“10,000).

If you give this spec to your implementation LLM, it has:

Clear data contracts

Clear modeling equations

UI flow

Explicit handling of None and competitor alternatives (your chosen Approaches A and B)

A clean separation between Python estimation and JS visualization/simulation.

If you want, next step I can generate:

Example dummy dataset + JSON metadata

Skeleton code structures:

Python /estimate handler

JS modules for simulation and visualization.